# AIMedBench: Rigorous Evaluation of AI/ML Tools in Rare Disease Genomics

This repository demonstrates **how to rigorously evaluate third-party AI/ML tools** for **rare disease diagnostics** â€” focusing on  
**reproducibility, fairness, clinical translation, and safety**. Itâ€™s intentionally lightweight so reviewers can run it quickly.

> âš¡ Note: This is an **evaluation framework**, not a method development project.  
> Demo baselines (e.g. random, position-based) are included only to illustrate the pipeline; the primary purpose is to benchmark **external AI/ML tools**.

---

## Whatâ€™s here
- `docs/` â€” concise design docs: tool landscape, evaluation framework, cloud/HPC feasibility.
- `workflow/` â€” a Snakemake pipeline running a reproducible demo benchmark end-to-end.
- `src/` â€” clear Python scripts: data IO, baselines (demo only), metric computation, and report generation.
- `data/` â€” tiny example datasets (synthetic + ClinVar-style) for fast demo runs.
- `notebooks/` â€” minimal notebooks for quick visualization and â€œdrop-in predictionsâ€ examples.
- `results/` â€” pipeline outputs: metrics tables, plots, and reports.

---

## ğŸ“‚ Repository Structure

```
AIMedBench/
â”œâ”€â”€ README.md                         # Project overview and usage instructions
â”œâ”€â”€ LICENSE                           # Open-source license (MIT)
â”œâ”€â”€ .gitignore                        # Files and directories ignored by Git
â”œâ”€â”€ pyproject.toml                     # Python project metadata and dependencies
â”œâ”€â”€ Dockerfile                        # Container image definition for reproducible runs
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ environment.yml               # Conda environment specification
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 01_landscape.md               # Survey of external AI/ML tools to evaluate
â”‚   â”œâ”€â”€ 02_framework.md               # Evaluation framework (datasets, metrics, clinical vs research priorities)
â”‚   â”œâ”€â”€ 03_feasibility.md             # Cloud/HPC feasibility and deployment considerations
â”‚   â””â”€â”€ Artificial_Intelligence_and_Machine_Learning_in_Bioinformatics-Kaitao_Lai.pdf
â”‚                                     # Reference paper (2018 book chapter)
â”œâ”€â”€ workflow/
â”‚   â””â”€â”€ Snakefile                     # Snakemake workflow for running benchmarks end-to-end
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py                      # Helper functions (e.g., build variant IDs)
â”‚   â”œâ”€â”€ predictions_random.py         # Demo baseline predictor (random scores)
â”‚   â”œâ”€â”€ predictions_position.py       # Demo baseline predictor (position-based heuristic)
â”‚   â”œâ”€â”€ aggregate.py                  # Functions to load and merge external tool predictions
â”‚   â”œâ”€â”€ evaluate.py                   # Metric computation (AUROC, AUPRC, Brier score, etc.)
â”‚   â”œâ”€â”€ report.py                     # Report generation (Research vs Clinical metrics sections)
â”‚   â””â”€â”€ main.py                       # Main pipeline script to run evaluation + reporting
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ simulated_variants.tsv        # Synthetic toy variants (demo input)
â”‚   â”œâ”€â”€ clinvar_tiny.tsv              # Tiny ClinVar-style dataset (pathogenic vs benign)
â”‚   â””â”€â”€ truth_labels.tsv              # Ground-truth labels for demo evaluation
â”œâ”€â”€ predictions/
â”‚   â””â”€â”€ external/
â”‚       â””â”€â”€ spliceai_sample.csv       # Example external tool predictions (SpliceAI demo)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ spliceai_demo.ipynb           # Jupyter notebook: evaluate SpliceAI predictions
â”‚   â””â”€â”€ benchmark_dashboard.ipynb     # Jupyter notebook: simple dashboard of results
â”œâ”€â”€ results/                          # Auto-generated outputs from pipeline
â”‚   â”œâ”€â”€ metrics.tsv                   # Evaluation metrics table
â”‚   â”œâ”€â”€ report.md                     # Markdown report with research vs clinical metrics
â”‚   â””â”€â”€ manifest.json                 # Provenance metadata (datasets, configs, outputs)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ gcp_config.yaml               # Example configuration for GCP/HPC deployment
â””â”€â”€ tests/
â””â”€â”€ test_metrics.py               # Unit test for metrics computation
```

---

## Evaluation focus
The framework distinguishes priorities in **two settings**:  

- **Research setting**:  
  - Discrimination metrics (AUROC, AUPRC)  
  - Robustness under domain shift  
  - Cost/latency profiling  

- **Clinical setting**:  
  - Calibration (Brier score, reliability curves)  
  - Sensitivity/specificity at clinically meaningful thresholds  
  - False positive/negative rates on benign/pathogenic variants  
  - Interpretability and reproducibility  

---

## The plan â†’ this repo (abridged)
- Curate candidate tools, validate inputs, define datasets & metrics â†’ see `docs/01_landscape.md` and `docs/02_framework.md`.
- Formalize benchmarks and robustness tests â†’ pipeline skeleton (`workflow/Snakefile`) + metric engine (`src/evaluate.py`).
- Clinical feasibility & safety sign-off â†’ `docs/03_feasibility.md` + generated reports in `results/`.

---

## Quickstart (local)
```bash
# Option A: conda
conda env create -f envs/environment.yml
conda activate rd-eval
python -m pip install -e .

# Run the demo benchmark
snakemake -j 1 --directory . --cores 1

# Option B: Docker
docker build -t rd-eval:latest .
docker run --rm -v $PWD:/work -w /work rd-eval:latest   bash -lc "snakemake -j 1 --directory . --cores 1"
```

---

## Where to plug real tools
- Place **external tool predictions** as TSV/CSV into `predictions/external/` with columns:  
  `variant_id,score,tool`  
- The pipeline will automatically aggregate and re-compute metrics side-by-side with baselines.  
- Example: `notebooks/spliceai_demo.ipynb` demonstrates loading SpliceAI predictions and evaluating them.

---

## ğŸ“– Reference & Background
This repo operationalises evaluation best-practices I co-authored in:  

**Lai et al. (2018) Artificial Intelligence and Machine Learning in Bioinformatics (Elsevier)**  

[![Read the paper](https://img.shields.io/badge/Paper-PDF-blue)](docs/Artificial_Intelligence_and_Machine_Learning_in_Bioinformatics-Kaitao_Lai.pdf)

Click to view the PDF.

---

_Repo last updated: 2025-08-25_
